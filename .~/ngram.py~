from nltk import *
import random, sys, re
import nltk
import argparse
import itertools
import Levenshtein
import time, datetime
import pickle
t = time.time()
import random, sys, re

user_celex_path = "/Users/km/Documents/celex2/"


"""produces file with header: "lexicon,homophones,mps,neighbors,avg_lev,num_words\n"

file name is: lemma, language, model, mono, homocel, minlength, maxlength, n, homophones, iterations
to do non-English, mono has to be and it's untested

can either use argument --corpus to pass a list of words from a txt file
or can specify parameters for celex to use

will produce folders celexes and rfiles
"""

try: 
    os.mkdir('rfiles')
except OSError: 
    pass

try:
    os.mkdir('celexes')
except OSError:
    pass

""" --------------word cleaning functions-------------- """
vowels = {}
for v in "IE{VQU@i#$u312456789cq0~iI1!eE2@aAoO3#4$6^uU7&5+8*9(0)<>[]{}":
    vowels[v] = 0

def clean_word(word):
    """Remove stress and syllable boundaries."""
    word = re.sub("'", "", word)
    word = re.sub('"', "", word)
    if args.syll != 1: word = re.sub("-", "", word)
    return word

def celex_diphthong_sub(word):
    """ Do celex dipthong subs. """
    word = re.sub("2", "#I", word)
    word = re.sub("4", "QI", word)
    word = re.sub("6", "#U", word)
    word = re.sub("7", "I@", word)
    word = re.sub("8", "E@", word)
    word = re.sub("9", "U@", word)
    return word
""" --------------Celex reading functions-------------- """

def get_cv(word):
    cv = ""
    for letter in word:
        if letter in vowels: 
            cv += "V"
        else: cv += "C"
    if args.cv == 1: return cv
    return len(word)

def read_in_celex_lines(path):
    """ Return lines in given celex file. """
    return [line.strip().split("\\") for line in open(path, "r").readlines()]


def get_celex_monos(path, language):
    """ Return list of celex ids for monomorphemes. """
    a = read_in_celex_lines(user_celex_path + "%s/eml/eml.cd" %language)
    return [i[0] for i in a if i[3] == "M"]

def get_celex_path(path, lemma, language):
    """ Return celex path, given root, lemma, language. """
    return path + language + "/{lang}p{lem}/{lang}p{lem}.cd".format(lang=language[0], lem=lemma[0])


def celex_pron_loc(language, lemma):
    """ Return location of pronunciation in celex, given language. """
    pron = 5
    if language == "german" or language== "dutch": pron = pron -2 #german one less
    if lemma == "wordform": pron += 1
    return pron

def extract_celex_info(line, language="english", lemma="lemma", model="ortho"):
    """ Return celex word (ortho or phonemic) and its freq from celex line. """
    if line[1].isalpha() and line[1].islower() and "-" not in line[1] and "." not in line[1] and "'" not in line[1] and " " not in line[1]:
        return line[celex_pron_loc(language, lemma)], line[2] #pron, frequency
    return


def build_celex_corpus(path, language, lemma, model, mono):
    """ Return corpus from celex, given path and parameters. """
    lines = read_in_celex_lines(path); corpus = []; monos = []
    if mono == 1: monos = get_celex_monos(path, language)
    corpus = [extract_celex_info(line, language, lemma, model) for line in lines if (mono == 0 or line[0] in monos)]
    return [i for i in corpus if i != None]


def build_real_lex(path, lemma, language, model, mono, homo, minlength, maxlength, celex_list):
    celex_path = get_celex_path(path, lemma, language)
    print celex_path
    corpus = build_celex_corpus(celex_path, language, lemma, model, mono)
    print ">>>TOTAL NB OF WORDS", len(corpus)
    corpus = [c for c in corpus if c[1] > 0]
    print ">>>TOTAL NB OF WORDS", len(corpus)
    corpus = [clean_word(c[0]) for c in corpus] #reduce celex to just pronunciation
    print ">>>TOTAL NB OF WORDS", len(corpus)
    corpus =  [celex_diphthong_sub(c) for c in corpus if "c" not in c and "q" not in c and "0" not in c and "~" not in c]
    print ">>>TOTAL NB OF WORDS", len(corpus)
    corpus = [i for i in corpus if (len(re.sub("-", "", i)) >= minlength and len(re.sub("-", "", i)) <= maxlength)]
    print ">>>TOTAL NB OF WORDS", len(corpus)
    if homo == 0: corpus = list(set(corpus))
    print ">>>TOTAL NB OF WORDS", len(corpus)    
    f = open("celexes/" + "_".join([str(i) for i in celex_list]) + ".txt", "w")
    for line in corpus: f.write(line + "\n")
    f.close()
    return corpus



""" --------------ngram model------------"""

def multichooser(context,fd):
    """ Return random choice from multinomial cfd. """
    context = "".join(context)
    cumprob = 0
    rand = random.random()
    possibles = fd[context].samples()
    possibles.sort()
    for possible in possibles:
        cumprob += fd[context].freq(possible)
        if cumprob > rand: return possible
    return



class NgramModel():
    def __init__(self, n, corpus, ngen, homo):
        self.__dict__.update(locals())
        self.cfd = ConditionalFreqDist()
        self.update_cfd(self.n, self.corpus)


    def update_cfd(self, n, corpus):
        """Update cfd using ngrams"""
        for item in corpus:
             item_ngrams = nltk.ngrams(["<S>"]*(n-1) + [i for i in item] + ["<E>"], n)
             for ng in item_ngrams:
                 self.cfd["".join(ng[:-1])].inc(ng[-1])
        return

    def generate_one(self, n, corpus):
        """Generate one word from ngram model."""
        word = ["<S>"]*(self.n - 1)
        while True:
            context = "".join(word[(len(word) - (self.n -1)):len(word)])
            word = word + [multichooser(context, self.cfd)]
            if word[-1] == "<E>":
                break
        return "".join(word[(self.n - 1):-1])

    def generate(self):
        """Generate as many words as specified by ngen"""
        words = [self.generate_one(self.n, self.corpus) for xx in range(self.ngen)]
        return words
        
    
                
def generate_correct_number(n, corpus_name, homo):
    """Generate number of words to match length, handle homophones being generated"""
    try:
        corpus = open(corpus_name, "r").readlines()
        corpus = [i.strip() for i in corpus]
    except TypeError:
        corpus = corpus_name
    x = NgramModel(n, corpus, 10, homo) #generate 10 at at a time
    lengths_needed = nltk.defaultdict(int)
    for item in corpus:
        lengths_needed[get_cv(item)] += 1
    newwords = []
    while True:
        words = x.generate()
        for w in words:
            if lengths_needed[get_cv(w)] > 0:
                if homo == 1 or w not in newwords:
                    lengths_needed[get_cv(w)] += -1
                    newwords += [w]
            elif sum([lengths_needed[j] for j in lengths_needed.keys()]) == 0: 
                return newwords

def write_lex_stats(b, num):
    """Use Levenshtein package to calcualte lev and count up mps, neighbors, etc"""
    total = 0.
    mps = 0
    neighbors = 0
    homophones = 0
    lev_total = 0
    ndict = nltk.defaultdict(int)
    mdict = nltk.defaultdict(int)
    hdict = nltk.defaultdict(int)
    for item in itertools.combinations(b, 2):
        lev = Levenshtein.distance(item[0], item[1])
        if lev == 0: 
            homophones += 1
            hdict[item[0]] += 1
        elif lev == 1: 
            neighbors += 1
            ndict[item[0]] += 1
            ndict[item[1]] += 1
            if len(item[0]) == len(item[1]): 
                mps += 1
                mdict[item[0]] += 1
                mdict[item[1]] += 1
        total += 1
        lev_total += lev
    print str(num)
    f.write(",".join([str(x) for x in [num, homophones, mps, neighbors, lev_total/total, len(b)] ]) + "\n")
    for item in b:
        f2.write(",".join([str(num), str(item), str(hdict[item]), str(mdict[item]/(hdict[item] + 1.)), str(ndict[item]/(hdict[item] + 1.)), str(len(item)) ]) + "\n")
    return
    
#build_real_lex(user_celex_path, "lemma", "english", "nphone", 0, 1, 3, 9
parser = argparse.ArgumentParser()
parser.add_argument('--n', metavar='--n', type=int, nargs='?',
                    help='n for ngram', default=3)
parser.add_argument('--homo', metavar='h', type=int, nargs='?',
                    help='0 to exclude homophones being generated, 1 for homophones allowed', default=0)
parser.add_argument('--lemma', metavar='--lem', type=str, nargs='?',
                    help='lemma or wordform in celex', default="lemma")
parser.add_argument('--language', metavar='--lang', type=str, nargs='?',
                    help='', default="english")
parser.add_argument('--model', metavar='--m', type=str, nargs='?',
                    help='should be nphone for ngram model', default="nphone")
parser.add_argument('--mono', metavar='--mono', type=int, nargs='?',
                    help='1 for mono only, 0 ow', default=1)
parser.add_argument('--homocel', metavar='--homocel', type=int, nargs='?',
                    help='1 for allow homo in celex, 0 ow', default=0)
parser.add_argument('--minlength', metavar='--minl', type=int, nargs='?',
                    help='minimum length of word allowed from celex', default=4)
parser.add_argument('--maxlength', metavar='--maxl', type=int, nargs='?',
                    help='maximum length of word allowed from celex', default=8)
parser.add_argument('--iter', metavar='--i', type=int, nargs='?',
                    help='number of lexicons to generate', default=2)
parser.add_argument('--corpus', metavar='--c', type=str, nargs='?', help='put corpus file (list of words, one on each lien) to override celex', default='celex')
parser.add_argument('--syll', metavar='--syll', type=int, nargs='?', help='include syll in celex', default=0)
parser.add_argument('--inputsim', metavar='--inputsim', type=str, nargs='?', help='use this if you want to input a file that contains simulated lexicons instead of ngrams. the file should be a csv in format simnum,word', default="none")
parser.add_argument('--cv', metavar='--cv', type=int, nargs='?', help='put 1 here to match for CV pattern', default=0)


args = parser.parse_args()
celex_list = [args.lemma, args.language, args.model, args.mono, args.homocel, args.minlength, args.maxlength]

if args.syll == 1: celex_list = ['syll_'] + celex_list

if args.corpus == 'celex': 
    a = build_real_lex(user_celex_path, args.lemma, args.language, args.model, args.mono, args.homocel, args.minlength, args.maxlength, celex_list)
    argslist = "_".join([str(j) for j in celex_list] + [str(args.n), str(args.homo), str(args.iter), str(args.cv)])

else: 
    if args.inputsim == 'none': 
        a = [line.strip() for line in open(args.corpus).readlines()]
        argslist = args.corpus.split("/")[-1] + str(args.cv)
    else: argslist = str(args.minlength) + "_" + str(args.maxlength) + "_" + str(args.cv)

outname = ""
if args.inputsim != "none":
    outname = args.inputsim.split("/")[-1][:-4]

f = open("rfiles/" + argslist + "_" + outname + ".txt", "w")
f.write("lexicon,homophones,mps,neighbors,avg_lev,num_words\n")

f2 = open("rfiles/" + "indwords_" + argslist + "_" + outname + ".txt", "w")
f2.write("lexicon,word,homophones,mps,neighbors,length\n")

if args.inputsim != "none":
    inputlist = nltk.defaultdict(list)
    b = [line.strip().split(",") for line in open(args.inputsim).readlines()]
    b = [line for line in b if len(line[1]) >= args.minlength and len(line[1]) <= args.maxlength]
    for item in b: #divide up lexicons
        inputlist[int(item[0])] += [item[1]]
    write_lex_stats(inputlist[-1], "real")
    for i in range(0, max(inputlist.keys()) + 1):
        write_lex_stats(inputlist[i], i)

else:
    write_lex_stats(a, "real")
    for it in range(args.iter):
        if args.corpus == 'celex': 
            write_lex_stats(generate_correct_number(args.n, "celexes/" + "_".join([str(i) for i in celex_list]) + ".txt", args.homo), it)
        else:
            write_lex_stats(generate_correct_number(args.n, args.corpus, args.homo), it)


f.close()
f2.close()


#python ngram.py --inputsim=permuted_syllssyll__lemma_english_nphone_1_0_4_8.txt --corpus=notcelex
