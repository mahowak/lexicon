\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{hockett1960origin}
\citation{hockett1960origin}
\citation{monaghan_arbitrariness_2011}
\citation{Gasser2004origins}
\citation{bergen_psychological_2004}
\citation{hinton2006sound,sapir1929study}
\citation{reilly_arbitrary_2012}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{shannon_mathematical_2001,levy_expectation-based_2008,gibson2013noisy}
\citation{flemming_contrast_2004,graff_2012}
\citation{shannon1948mathematical}
\citation{hockett1955manual,flemming2002auditory}
\citation{steriade1997phonetics,steriade2001directional}
\citation{hayes_blick_2012}
\citation{odonnell_productivity_2011}
\citation{monaghan_arbitrariness_2011}
\citation{storkel_differentiating_2006}
\citation{mandelbrot_informational,miller1957some}
\citation{howes1968zipf,piantadosi2012information}
\citation{luce1986neighborhoods,luce1998recognizing}
\citation{vitevitch1998words}
\citation{prabhakaran2006event}
\@writefile{toc}{\contentsline {section}{\numberline {2}Neighborhood and frequency effects}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Method}{3}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Results}{4}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Orthographic probability plotted against frequency. The red line is a line of best fit.\relax }}{4}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{probvfreq}{{1}{4}{Orthographic probability plotted against frequency. The red line is a line of best fit.\relax \relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Number of orthographic neighbors plotted against frequency. The red line is a line of best fit.\relax }}{5}{figure.caption.2}}
\newlabel{neighborsvfreq}{{2}{5}{Number of orthographic neighbors plotted against frequency. The red line is a line of best fit.\relax \relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Each bar shows the mean correlation across languages for the variable shown. ``n'' here refers to number of neighbors, ``f'' is log frequency, ``mp'' is minimal pairs, and ``prob'' is orthographic probability as measured by the 3-phone model.\relax }}{5}{figure.caption.3}}
\newlabel{cor.test}{{3}{5}{Each bar shows the mean correlation across languages for the variable shown. ``n'' here refers to number of neighbors, ``f'' is log frequency, ``mp'' is minimal pairs, and ``prob'' is orthographic probability as measured by the 3-phone model.\relax \relax }{figure.caption.3}{}}
\citation{weide1998cmu}
\citation{hayes_blick_2012}
\newlabel{cor.table}{{\caption@xref {cor.table}{ on input line 220}}{6}{Results\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Discussion}{6}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Null lexicon experiments}{6}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Lexicons}{6}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Choosing the best null model}{7}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Method}{7}{subsubsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Results}{7}{subsubsection.3.2.2}}
\citation{graff_2012}
\citation{marslen1980speech,marslen1987functional,wingfield1997word}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Results}{8}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Minimal pairs}{8}{subsubsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Word onset measures}{8}{subsubsection.3.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Each colored histogram represents a distribution of minimal pair counts, broken up by word length in phonemes, across the 50 simulated lexicons from each of the four generative models. The y-axis shows the log number of minimal pairs per 10,000 words for each of the four types of lexicons. The red dot shows the real lexicon value for each condition. The most tightly constrained generative model (CV and phonotactically matched) comes closest to matching the real lexicon value in all cases, but all of the histograms for all of the lengths fall to the left of the red dot, which suggests that the real lexicon has more minimal pairs than any of the simulated ones.\relax }}{9}{figure.caption.5}}
\newlabel{fig:pairs}{{4}{9}{Each colored histogram represents a distribution of minimal pair counts, broken up by word length in phonemes, across the 50 simulated lexicons from each of the four generative models. The y-axis shows the log number of minimal pairs per 10,000 words for each of the four types of lexicons. The red dot shows the real lexicon value for each condition. The most tightly constrained generative model (CV and phonotactically matched) comes closest to matching the real lexicon value in all cases, but all of the histograms for all of the lengths fall to the left of the red dot, which suggests that the real lexicon has more minimal pairs than any of the simulated ones.\relax \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces These histograms show the distribution of the most closely matched simulated lexicon (in green) compared to the real lexicon (the red dot) in terms of confusable minimal pairs: `p/b', `k/g', and `t/d'. The dotted lines represent 95\% confidence intervals derived from the distribution of simulated lexicons. In all cases, the red dot is significantly to the right of the simulated lexicon distribution--which suggests that the real lexicon is clumpier than expected by chance. \relax }}{10}{figure.caption.6}}
\newlabel{3pairs}{{5}{10}{These histograms show the distribution of the most closely matched simulated lexicon (in green) compared to the real lexicon (the red dot) in terms of confusable minimal pairs: `p/b', `k/g', and `t/d'. The dotted lines represent 95\% confidence intervals derived from the distribution of simulated lexicons. In all cases, the red dot is significantly to the right of the simulated lexicon distribution--which suggests that the real lexicon is clumpier than expected by chance. \relax \relax }{figure.caption.6}{}}
\citation{arbesman_structure_2010}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces NEED \relax }}{11}{figure.caption.7}}
\newlabel{3onset}{{6}{11}{NEED \relax \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Levenshtein distance}{11}{subsubsection.3.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The histograms show the distribution of average Levenshtein distances for each of the 50 simulated lexicons (restricted to only the most closely matched generative model). The red dot represents the real lexicon's value, and the dotted lines are 95\% confidence intervals.\relax }}{12}{figure.caption.8}}
\newlabel{levs}{{7}{12}{The histograms show the distribution of average Levenshtein distances for each of the 50 simulated lexicons (restricted to only the most closely matched generative model). The red dot represents the real lexicon's value, and the dotted lines are 95\% confidence intervals.\relax \relax }{figure.caption.8}{}}
\citation{wasserman1994social,watts1998collective,barabasi1999emergence}
\citation{fellbaum_wordnet_2010}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Network measures}{13}{subsubsection.3.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Effect of semantics}{13}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Example phonological network. Each word is a node, and any words that are 1 edit apart are connected by an edge.\relax }}{14}{figure.caption.9}}
\newlabel{fig:toy}{{8}{14}{Example phonological network. Each word is a node, and any words that are 1 edit apart are connected by an edge.\relax \relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Sampling of phonological neighbor network from the different models. Each point is a word, and any two connected words are phonological neighbors. The simulated lexicons from less constrained generative models are less clustered and have more isolates (words with no neighbors, plotted on the outside ring).\relax }}{15}{figure.caption.10}}
\newlabel{fig:networks}{{9}{15}{Sampling of phonological neighbor network from the different models. Each point is a word, and any two connected words are phonological neighbors. The simulated lexicons from less constrained generative models are less clustered and have more isolates (words with no neighbors, plotted on the outside ring).\relax \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces These histograms show the distribution of the most closely matched simulated lexicon (in green) compared to the real lexicon (the red dot) in terms of network measures for lexical networks (where each node is a word and any 2 nodes that are minimal pairs are joined in the network): the percent of nodes in the giant component, the average clustering coefficient, and transitivity. In all cases, the red dot is significantly to the right of the simulated lexicon distribution--which suggests that the real lexicon is clumpier than expected by chance. \relax }}{16}{figure.caption.11}}
\newlabel{3net}{{10}{16}{These histograms show the distribution of the most closely matched simulated lexicon (in green) compared to the real lexicon (the red dot) in terms of network measures for lexical networks (where each node is a word and any 2 nodes that are minimal pairs are joined in the network): the percent of nodes in the giant component, the average clustering coefficient, and transitivity. In all cases, the red dot is significantly to the right of the simulated lexicon distribution--which suggests that the real lexicon is clumpier than expected by chance. \relax \relax }{figure.caption.11}{}}
\citation{storkel_differentiating_2006}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{17}{section.5}}
\citation{piantadosi_communicative_2012}
\bibstyle{apacite}
\bibdata{/Users/km/Dropbox/lyx/refs}
\bibcite{arbesman_structure_2010}{{1}{{\APACyear {2010}}}{{Arbesman\ \BOthers {.}}}{{Arbesman, Strogatz,{}\ \&{} Vitevitch}}}
\bibcite{baayen_celex_1995}{{2}{{\APACyear {1995}}}{{Baayen\ \BOthers {.}}}{{Baayen, Piepenbrock,{}\ \&{} Gulikers}}}
\bibcite{barabasi1999emergence}{{3}{{\APACyear {1999}}}{{Barab{\'a}si\ \&{} Albert}}{{Barab{\'a}si\ \&{} Albert}}}
\bibcite{bergen_psychological_2004}{{4}{{\APACyear {2004}}}{{Bergen}}{{Bergen}}}
\bibcite{fellbaum_wordnet_2010}{{5}{{\APACyear {2010}}}{{Fellbaum}}{{Fellbaum}}}
\bibcite{flemming2002auditory}{{6}{{\APACyear {2002}}}{{Flemming}}{{Flemming}}}
\bibcite{flemming_contrast_2004}{{7}{{\APACyear {2004}}}{{Flemming}}{{Flemming}}}
\bibcite{Gasser2004origins}{{8}{{\APACyear {2004}}}{{Gasser}}{{Gasser}}}
\bibcite{gibson2013noisy}{{9}{{\APACyear {2013}}}{{Gibson\ \BOthers {.}}}{{Gibson, Bergen,{}\ \&{} Piantadosi}}}
\bibcite{graff_2012}{{10}{{\APACyear {2012}}}{{Graff}}{{Graff}}}
\bibcite{hayes_blick_2012}{{11}{{\APACyear {2012}}}{{Hayes}}{{Hayes}}}
\bibcite{hinton2006sound}{{12}{{\APACyear {2006}}}{{Hinton\ \BOthers {.}}}{{Hinton, Nichols,{}\ \&{} Ohala}}}
\bibcite{hockett1960origin}{{13}{{\APACyear {1960}}}{{Hockett}}{{Hockett}}}
\bibcite{hockett1955manual}{{14}{{\APACyear {1955}}}{{Hockett\ \&{} Voegelin}}{{Hockett\ \&{} Voegelin}}}
\bibcite{howes1968zipf}{{15}{{\APACyear {1968}}}{{Howes}}{{Howes}}}
\bibcite{levy_expectation-based_2008}{{16}{{\APACyear {2008}}}{{Levy}}{{Levy}}}
\@writefile{toc}{\contentsline {section}{References}{19}{section*.12}}
\bibcite{luce1986neighborhoods}{{17}{{\APACyear {1986}}}{{Luce}}{{Luce}}}
\bibcite{luce1998recognizing}{{18}{{\APACyear {1998}}}{{Luce\ \&{} Pisoni}}{{Luce\ \&{} Pisoni}}}
\bibcite{mandelbrot_informational}{{19}{{\APACyear {{\bibnodate {}}}}}{{Mandelbrot}}{{Mandelbrot}}}
\bibcite{marslen1980speech}{{20}{{\APACyear {1980}}}{{Marslen-Wilson}}{{Marslen-Wilson}}}
\bibcite{marslen1987functional}{{21}{{\APACyear {1987}}}{{Marslen-Wilson}}{{Marslen-Wilson}}}
\bibcite{miller1957some}{{22}{{\APACyear {1957}}}{{Miller}}{{Miller}}}
\bibcite{monaghan_arbitrariness_2011}{{23}{{\APACyear {2011}}}{{Monaghan\ \BOthers {.}}}{{Monaghan, Christiansen,{}\ \&{} Fitneva}}}
\bibcite{odonnell_productivity_2011}{{24}{{\APACyear {2011}}}{{{O'Donnell}}}{{{O'Donnell}}}}
\bibcite{piantadosi_communicative_2012}{{25}{{\APACyear {2012}}}{{Piantadosi\ \BOthers {.}}}{{Piantadosi, Tily,{}\ \&{} Gibson}}}
\bibcite{piantadosi2012information}{{26}{{\APACyear {Under review}}}{{Piantadosi\ \BOthers {.}}}{{Piantadosi, Tily,{}\ \&{} Gibson}}}
\bibcite{prabhakaran2006event}{{27}{{\APACyear {2006}}}{{Prabhakaran\ \BOthers {.}}}{{Prabhakaran, Blumstein, Myers, Hutchison,{}\ \&{} Britton}}}
\bibcite{reilly_arbitrary_2012}{{28}{{\APACyear {2012}}}{{Reilly\ \BOthers {.}}}{{Reilly, Westbury, Kean,{}\ \&{} Peelle}}}
\bibcite{sapir1929study}{{29}{{\APACyear {1929}}}{{Sapir}}{{Sapir}}}
\bibcite{shannon_mathematical_2001}{{30}{{\APACyear {1948}}{\APACexlab {{\BCnt {1}}}}}{{Shannon}}{{Shannon}}}
\bibcite{shannon1948mathematical}{{31}{{\APACyear {1948}}{\APACexlab {{\BCnt {2}}}}}{{Shannon}}{{Shannon}}}
\bibcite{steriade1997phonetics}{{32}{{\APACyear {1997}}}{{Steriade}}{{Steriade}}}
\bibcite{steriade2001directional}{{33}{{\APACyear {2001}}}{{Steriade}}{{Steriade}}}
\bibcite{storkel_differentiating_2006}{{34}{{\APACyear {2006}}}{{Storkel\ \BOthers {.}}}{{Storkel, Armbruster,{}\ \&{} Hogan}}}
\bibcite{vitevitch1998words}{{35}{{\APACyear {1998}}}{{Vitevitch\ \&{} Luce}}{{Vitevitch\ \&{} Luce}}}
\bibcite{wasserman1994social}{{36}{{\APACyear {1994}}}{{Wasserman\ \&{} Faust}}{{Wasserman\ \&{} Faust}}}
\bibcite{watts1998collective}{{37}{{\APACyear {1998}}}{{Watts\ \&{} Strogatz}}{{Watts\ \&{} Strogatz}}}
\bibcite{weide1998cmu}{{38}{{\APACyear {1998}}}{{Weide}}{{Weide}}}
\bibcite{wingfield1997word}{{39}{{\APACyear {1997}}}{{Wingfield\ \BOthers {.}}}{{Wingfield, Goodglass,{}\ \&{} Lindfield}}}
